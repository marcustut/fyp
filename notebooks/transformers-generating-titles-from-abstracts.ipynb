{"cells":[{"cell_type":"markdown","metadata":{},"source":["## ðŸ¤—Transformers - Generating Articles from Paper's Abstracts using T5 Model\n","This notebook uses T5 model - A Sequence to Sequence model fully capable to perform any text to text tasks. What does it mean - It means that T5 model can take any input text and convert it into any output text. Such Text to Text conversion is useful in NLP tasks like language translation, summarization etc.\n","\n","In this notebook, we will take paper's abstracts as our input text and paper's title as output text and feed it to T5 model. So,let's dive in...\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["We will install dependencies and work with latest stable pytorch 1.6"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["# !pip install -U simpletransformers  "]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","\n","# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["import os, psutil  \n","\n","def cpu_stats():\n","    pid = os.getpid()\n","    py = psutil.Process(pid)\n","    memory_use = py.memory_info()[0] / 2. ** 30\n","    return 'memory GB:' + str(np.round(memory_use, 2))"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["'memory GB:0.1'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["cpu_stats()"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["import json\n","\n","data_file = '../dataset/arxiv-metadata-oai-snapshot.json'\n","\n","def get_metadata():\n","    with open(data_file, 'r') as f:\n","        for line in f:\n","            yield line"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Title: Calculation of prompt diphoton production cross sections at Tevatron and\n","  LHC energies\n","\n","Abstract:   A fully differential calculation in perturbative quantum chromodynamics is\n","presented for the production of massive photon pairs at hadron colliders. All\n","next-to-leading order perturbative contributions from quark-antiquark,\n","gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\n","all-orders resummation of initial-state gluon radiation valid at\n","next-to-next-to-leading logarithmic accuracy. The region of phase space is\n","specified in which the calculation is most reliable. Good agreement is\n","demonstrated with data from the Fermilab Tevatron, and predictions are made for\n","more detailed tests with CDF and DO data. Predictions are shown for\n","distributions of diphoton pairs produced at the energy of the Large Hadron\n","Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\n","boson are contrasted with those produced from QCD processes at the LHC, showing\n","that enhanced sensitivity to the signal can be obtained with judicious\n","selection of events.\n","\n","Ref: Phys.Rev.D76:013009,2007\n"]}],"source":["metadata = get_metadata()\n","for paper in metadata:\n","    paper_dict = json.loads(paper)\n","    print('Title: {}\\n\\nAbstract: {}\\nRef: {}'.format(paper_dict.get('title'), paper_dict.get('abstract'), paper_dict.get('journal-ref')))\n","#     print(paper)\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["**We will take last 5 years ArXiv papers (2016-2021) due to Kaggle'c compute limits**"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["(18569, 18569, 18569)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["titles = []\n","abstracts = []\n","years = []\n","metadata = get_metadata()\n","for paper in metadata:\n","    paper_dict = json.loads(paper)\n","    ref = paper_dict.get('journal-ref')\n","    try:\n","        year = int(ref[-4:]) \n","        if 2016 < year < 2021:\n","            years.append(year)\n","            titles.append(paper_dict.get('title'))\n","            abstracts.append(paper_dict.get('abstract'))\n","    except:\n","        pass \n","\n","len(titles), len(abstracts), len(years)"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>abstract</th>\n","      <th>year</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>On the Cohomological Derivation of Yang-Mills ...</td>\n","      <td>We present a brief review of the cohomologic...</td>\n","      <td>2017</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Regularity of solutions of the isoperimetric p...</td>\n","      <td>In this work we consider a question in the c...</td>\n","      <td>2018</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Asymptotic theory of least squares estimators ...</td>\n","      <td>This paper considers the effect of least squ...</td>\n","      <td>2017</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Teichm\\\"uller Structures and Dual Geometric Gi...</td>\n","      <td>The Gibbs measure theory for smooth potentia...</td>\n","      <td>2020</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Distributional Schwarzschild Geometry from non...</td>\n","      <td>In this paper we leave the neighborhood of t...</td>\n","      <td>2018</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               title  \\\n","0  On the Cohomological Derivation of Yang-Mills ...   \n","1  Regularity of solutions of the isoperimetric p...   \n","2  Asymptotic theory of least squares estimators ...   \n","3  Teichm\\\"uller Structures and Dual Geometric Gi...   \n","4  Distributional Schwarzschild Geometry from non...   \n","\n","                                            abstract  year  \n","0    We present a brief review of the cohomologic...  2017  \n","1    In this work we consider a question in the c...  2018  \n","2    This paper considers the effect of least squ...  2017  \n","3    The Gibbs measure theory for smooth potentia...  2020  \n","4    In this paper we leave the neighborhood of t...  2018  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["papers = pd.DataFrame({\n","    'title': titles,\n","    'abstract': abstracts,\n","    'year': years\n","})\n","papers.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["del titles, abstracts, years"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["'memory GB:0.13'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["cpu_stats()"]},{"cell_type":"markdown","metadata":{},"source":[" **We will use `simpletransformers` library to train a T5 model**"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["import logging\n","logging.basicConfig(level=logging.INFO)\n","transformers_logger = logging.getLogger(\"transformers\")\n","transformers_logger.setLevel(logging.WARNING)\n"]},{"cell_type":"markdown","metadata":{},"source":["**Simpletransformers implementation of T5 model expects a data to be a dataframe with 3 columns:**\n","`<prefix>, <input_text>, <target_text>`\n","* `<prefix>`: A string indicating the task to perform. (E.g. \"question\", \"stsb\")\n","* `<input_text>`: The input text sequence (we will use Paper's abstract as `input_text`  )\n","* `<target_text`: The target sequence (we will use Paper's title as `output_text` )\n","    \n","    \n"," You can read about the data format:  https://github.com/ThilinaRajapakse/simpletransformers#t5-transformer"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["papers = papers[['title','abstract']]\n","papers.columns = ['target_text', 'input_text']\n","papers = papers.dropna()"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["eval_df = papers.sample(frac=0.2, random_state=101)\n","train_df = papers.drop(eval_df.index)"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["((14855, 2), (3714, 2))"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["train_df.shape, eval_df.shape"]},{"cell_type":"markdown","metadata":{},"source":["**We will training out T5 model with very bare minimum `num_train_epochs=4`, `train_batch_size=16` to  fit into Kaggle's compute limits**"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.17k/1.17k [00:00<00:00, 402kB/s]\n","Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231M/231M [00:02<00:00, 85.0MB/s] \n","Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 773k/773k [00:01<00:00, 497kB/s]  \n","Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.32M/1.32M [00:01<00:00, 743kB/s] \n","INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14855/14855 [00:16<00:00, 888.67it/s] \n","INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-small_cached_51214855\n","INFO:simpletransformers.t5.t5_model: Training started\n","Epoch 1 of 4:   0%|          | 0/4 [00:00<?, ?it/s]C:\\Users\\E\\anaconda3\\envs\\slide-gen\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","Epochs 0/4. Running Loss:    2.5157: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 929/929 [05:08<00:00,  3.01it/s]\n","Epochs 1/4. Running Loss:    2.4712: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 929/929 [04:48<00:00,  3.23it/s]\n","Epochs 2/4. Running Loss:    2.0005: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 929/929 [04:54<00:00,  3.15it/s]\n","Epochs 3/4. Running Loss:    1.7137: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 929/929 [04:48<00:00,  3.22it/s]\n","Epoch 4 of 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [19:41<00:00, 295.30s/it]\n","INFO:simpletransformers.t5.t5_model: Training of t5-small model complete. Saved to outputs/.\n","INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3714/3714 [00:09<00:00, 376.60it/s]\n","INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-small_cached_5123714\n","Running Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 465/465 [00:24<00:00, 18.78it/s]\n","INFO:simpletransformers.t5.t5_model:{'eval_loss': 2.1558538724017398}\n"]}],"source":["import logging\n","\n","import pandas as pd\n","from simpletransformers.t5 import T5Model\n","\n","logging.basicConfig(level=logging.INFO)\n","transformers_logger = logging.getLogger(\"transformers\")\n","transformers_logger.setLevel(logging.WARNING)\n","\n","train_df['prefix'] = \"summarize\"\n","eval_df['prefix'] = \"summarize\"\n","\n","\n","model_args = {\n","    \"reprocess_input_data\": True,\n","    \"overwrite_output_dir\": True,\n","    \"max_seq_length\": 512,\n","    \"train_batch_size\": 16,\n","    \"num_train_epochs\": 4,\n","}\n","\n","# Create T5 Model\n","model = T5Model(model_name=\"t5-small\", model_type=\"t5\", args=model_args, use_cuda=True)\n","\n","# Train T5 Model on new task\n","model.train_model(train_df)\n","\n","# Evaluate T5 Model on new task\n","results = model.eval_model(eval_df)\n","\n","# Predict with trained T5 model\n","#print(model.predict([\"convert: four\"]))"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["from torch import cuda\n","cuda.is_available()"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["{'eval_loss': 2.1558538724017398}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["results"]},{"cell_type":"markdown","metadata":{},"source":["## And We're Done ! \n","**Let's see how our model performs in generating paper's titles**"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]C:\\Users\\E\\anaconda3\\envs\\slide-gen\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3426: FutureWarning: \n","`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n","`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n","your targets.\n","\n","Here is a short example:\n","\n","model_inputs = tokenizer(src_texts, ...)\n","with tokenizer.as_target_tokenizer():\n","    labels = tokenizer(tgt_texts, ...)\n","model_inputs[\"labels\"] = labels[\"input_ids\"]\n","\n","See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n","For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n","\n","  warnings.warn(formatted_warning, FutureWarning)\n","Generating outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12.00it/s]\n","Decoding outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.94s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Actual Title: Integral Hodge conjecture for Fermat varieties\n","Predicted Title: ['Lattice of Hodge cycles']\n","Actual Abstract: ['summarize:   We describe an algorithm which verifies whether linear algebraic cycles of\\nthe Fermat variety generate the lattice of Hodge cycles. A computer\\nimplementation of this confirms the integral Hodge conjecture for quartic and\\nquintic Fermat fourfolds. Our algorithm is based on computation of the list of\\nelementary divisors of both the lattice of linear algebraic cycles, and the\\nlattice of Hodge cycles written in terms of vanishing cycles, and observing\\nthat these two lists are the same.\\n']\n"]}],"source":["random_num = 350\n","actual_title = eval_df.iloc[random_num]['target_text']\n","actual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']]\n","predicted_title = model.predict(actual_abstract)\n","\n","print(f'Actual Title: {actual_title}')\n","print(f'Predicted Title: {predicted_title}')\n","print(f'Actual Abstract: {actual_abstract}')\n"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.50it/s]\n","Decoding outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.96s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Actual Title: Construction of genuine multipartite entangled states\n","Predicted Title: ['A Novel Product for Real Multipartite Entanglement']\n","Actual Abstract: ['summarize:   Genuine multipartite entanglement is of great importance in quantum\\ninformation, especially from the experimental point of view. Nevertheless, it\\nis difficult to construct genuine multipartite entangled states systematically,\\nbecause the genuine multipartite entanglement is unruly. We propose another\\nproduct based on the Kronecker product in this paper. The Kronecker product is\\na common product in quantum information with good physical interpretation. We\\nmainly investigate whether the proposed product of two genuine multipartite\\nentangled states is still a genuine entangled one. We understand the\\nentanglement of the proposed product better by characterizing the entanglement\\nof the Kronecker product. Then we show the proposed product is a genuine\\nmultipartite entangled state in two cases. The results provide a systematical\\nmethod to construct genuine multipartite entangled states of more parties.\\n']\n"]}],"source":["random_num = 478\n","actual_title = eval_df.iloc[random_num]['target_text']\n","actual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']]\n","predicted_title = model.predict(actual_abstract)\n","\n","print(f'Actual Title: {actual_title}')\n","print(f'Predicted Title: {predicted_title}')\n","print(f'Actual Abstract: {actual_abstract}')"]},{"cell_type":"code","execution_count":20,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.47it/s]\n","Decoding outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.86s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Actual Title: Automating Motion Correction in Multishot MRI Using Generative\n","  Adversarial Networks\n","Predicted Title: ['GAN: Generative Adversarial Network for Multishot Magnetic Resonance Imaging']\n","Actual Abstract: ['summarize:   Multishot Magnetic Resonance Imaging (MRI) has recently gained popularity as\\nit accelerates the MRI data acquisition process without compromising the\\nquality of final MR image. However, it suffers from motion artifacts caused by\\npatient movements which may lead to misdiagnosis. Modern state-of-the-art\\nmotion correction techniques are able to counter small degree motion, however,\\ntheir adoption is hindered by their time complexity. This paper proposes a\\nGenerative Adversarial Network (GAN) for reconstructing motion free\\nhigh-fidelity images while reducing the image reconstruction time by an\\nimpressive two orders of magnitude.\\n']\n"]}],"source":["random_num = 999\n","actual_title = eval_df.iloc[random_num]['target_text']\n","actual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']]\n","predicted_title = model.predict(actual_abstract)\n","\n","print(f'Actual Title: {actual_title}')\n","print(f'Predicted Title: {predicted_title}')\n","print(f'Actual Abstract: {actual_abstract}')"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]C:\\Users\\E\\anaconda3\\envs\\slide-gen\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3426: FutureWarning: \n","`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n","`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n","your targets.\n","\n","Here is a short example:\n","\n","model_inputs = tokenizer(src_texts, ...)\n","with tokenizer.as_target_tokenizer():\n","    labels = tokenizer(tgt_texts, ...)\n","model_inputs[\"labels\"] = labels[\"input_ids\"]\n","\n","See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n","For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n","\n","  warnings.warn(formatted_warning, FutureWarning)\n","Generating outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.62it/s]\n","Decoding outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.68s/it]\n"]},{"name":"stdout","output_type":"stream","text":["['Rococo Rococo: A Novel Design and a Novel Design']\n"]}],"source":["actual_abstract = [\"summarize: \"+\"\"\"Rococo Rococo (/rÉ™ËˆkoÊŠkoÊŠ/, also US: /ËŒroÊŠkÉ™ËˆkoÊŠ/), less commonly Roccoco or Late Baroque, is an exceptionally ornamental and theatrical style of architecture, art and decoration which combines asymmetry, scrolling curves, gilding, white and pastel colors, sculpted molding, and trompe-l'Å“il frescoes to create surprise and the illusion of motion and drama. 8][9] In the late 17th and early 18th century rocaille became the term for a kind of decorative motif or ornament that appeared in the late Style Louis XIV, in the form of a seashell interlaced with acanthus leaves.\"\"\"]\n","\n","print(model.predict(actual_abstract))"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pickle.dump(model, open('../models/title-generator.pkl', 'wb'))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["loaded_model = pickle.load(open('../models/title-generator-t5-arxiv-16-4.pkl', 'rb'))"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]C:\\Users\\E\\anaconda3\\envs\\slide-gen\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3426: FutureWarning: \n","`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n","`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n","your targets.\n","\n","Here is a short example:\n","\n","model_inputs = tokenizer(src_texts, ...)\n","with tokenizer.as_target_tokenizer():\n","    labels = tokenizer(tgt_texts, ...)\n","model_inputs[\"labels\"] = labels[\"input_ids\"]\n","\n","See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n","For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n","\n","  warnings.warn(formatted_warning, FutureWarning)\n","Generating outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.83it/s]\n","Decoding outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.03s/it]\n"]},{"data":{"text/plain":["['The Venetian Commodes']"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["loaded_model.predict(actual_abstract)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14.00it/s]\n","Decoding outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.87s/it]\n"]},{"data":{"text/plain":["['The Venetian Commodes']"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["actual_abstract = [\"summarize: \"+\"\"\"Venetian commodes imitated the curving lines and carved ornament of the French rocaille, but with a particular Venetian variation; the pieces were painted, often with landscapes or flowers or scenes from Guardi or other painters, or Chinoiserie, against a blue or green background, matching the colours of the Venetian school of painters whose work decorated the salons. 24] Ceiling of church of Santi Giovanni e Paolo in Venice, by Piazzetta (1727) Juno and Luna by Giovanni Battista Tiepolo (1735â€“45) Murano glass chandelier at the Ca Rezzonico (1758) Ballroom ceiling of the Ca Rezzonico with ceiling by Giovanni Battista Crosato (1753) In church construction, especially in the southern German-Austrian region, gigantic spatial creations are sometimes created for practical reasons alone, which, however, do not appear monumental, but are characterized by a unique fusion of architecture, painting, stucco, etc. ,.\"\"\"]\n","\n","loaded_model.predict(actual_abstract)"]},{"cell_type":"markdown","metadata":{},"source":["## Retrain Using Diff Parameters"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14855/14855 [00:22<00:00, 647.40it/s] \n","INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-small_cached_51214855\n","INFO:simpletransformers.t5.t5_model: Training started\n","Epochs 0/4. Running Loss:    2.5037: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 413/413 [04:25<00:00,  1.56it/s]\n","Epochs 1/4. Running Loss:    2.7283: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 413/413 [04:25<00:00,  1.56it/s]\n","Epochs 2/4. Running Loss:    1.5897: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 413/413 [04:25<00:00,  1.55it/s]\n","Epochs 3/4. Running Loss:    1.8895: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 413/413 [04:25<00:00,  1.56it/s]\n","Epoch 4 of 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [17:43<00:00, 265.80s/it]\n","INFO:simpletransformers.t5.t5_model: Training of t5-small model complete. Saved to outputs/.\n","INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3714/3714 [00:07<00:00, 469.86it/s]\n","INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-small_cached_5123714\n","Running Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 465/465 [00:26<00:00, 17.42it/s]\n","INFO:simpletransformers.t5.t5_model:{'eval_loss': 2.1366923239923294}\n"]}],"source":["import logging\n","\n","import pandas as pd\n","from simpletransformers.t5 import T5Model\n","\n","logging.basicConfig(level=logging.INFO)\n","transformers_logger = logging.getLogger(\"transformers\")\n","transformers_logger.setLevel(logging.WARNING)\n","\n","train_df['prefix'] = \"summarize\"\n","eval_df['prefix'] = \"summarize\"\n","\n","\n","model_args = {\n","    \"reprocess_input_data\": True,\n","    \"overwrite_output_dir\": True,\n","    \"max_seq_length\": 512,\n","    \"train_batch_size\": 36,\n","    \"num_train_epochs\": 4,\n","}\n","\n","# Create T5 Model\n","model = T5Model(model_name=\"t5-small\", model_type=\"t5\", args=model_args, use_cuda=True)\n","\n","# Train T5 Model on new task\n","model.train_model(train_df)\n","\n","# Evaluate T5 Model on new task\n","results = model.eval_model(eval_df)\n","\n","# Predict with trained T5 model\n","#print(model.predict([\"convert: four\"]))"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.67it/s]\n","Decoding outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:11<00:00, 11.73s/it]\n"]},{"name":"stdout","output_type":"stream","text":["['Rococo Rococo: Roccoco, Late Baroque, and']\n"]}],"source":["actual_abstract = [\"summarize: \"+\"\"\"Rococo Rococo (/rÉ™ËˆkoÊŠkoÊŠ/, also US: /ËŒroÊŠkÉ™ËˆkoÊŠ/), less commonly Roccoco or Late Baroque, is an exceptionally ornamental and theatrical style of architecture, art and decoration which combines asymmetry, scrolling curves, gilding, white and pastel colors, sculpted molding, and trompe-l'Å“il frescoes to create surprise and the illusion of motion and drama. 8][9] In the late 17th and early 18th century rocaille became the term for a kind of decorative motif or ornament that appeared in the late Style Louis XIV, in the form of a seashell interlaced with acanthus leaves.\"\"\"]\n","\n","print(model.predict(actual_abstract))"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]C:\\Users\\E\\anaconda3\\envs\\slide-gen\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3426: FutureWarning: \n","`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n","`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n","your targets.\n","\n","Here is a short example:\n","\n","model_inputs = tokenizer(src_texts, ...)\n","with tokenizer.as_target_tokenizer():\n","    labels = tokenizer(tgt_texts, ...)\n","model_inputs[\"labels\"] = labels[\"input_ids\"]\n","\n","See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n","For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n","\n","  warnings.warn(formatted_warning, FutureWarning)\n","Generating outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.42it/s]\n","Decoding outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.66s/it]\n"]},{"data":{"text/plain":["['The Venetian Commodes and the Renaissance']"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["actual_abstract = [\"summarize: \"+\"\"\"Venetian commodes imitated the curving lines and carved ornament of the French rocaille, but with a particular Venetian variation; the pieces were painted, often with landscapes or flowers or scenes from Guardi or other painters, or Chinoiserie, against a blue or green background, matching the colours of the Venetian school of painters whose work decorated the salons. 24] Ceiling of church of Santi Giovanni e Paolo in Venice, by Piazzetta (1727) Juno and Luna by Giovanni Battista Tiepolo (1735â€“45) Murano glass chandelier at the Ca Rezzonico (1758) Ballroom ceiling of the Ca Rezzonico with ceiling by Giovanni Battista Crosato (1753) In church construction, especially in the southern German-Austrian region, gigantic spatial creations are sometimes created for practical reasons alone, which, however, do not appear monumental, but are characterized by a unique fusion of architecture, painting, stucco, etc. ,.\"\"\"]\n","\n","model.predict(actual_abstract)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["pickle.dump(model, open('../models/title-generator-t5-arxiv-36-4.pkl', 'wb'))"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14855/14855 [00:16<00:00, 923.34it/s] \n","INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-small_cached_51214855\n","INFO:simpletransformers.t5.t5_model: Training started\n","Epochs 0/8. Running Loss:    2.3553: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 413/413 [04:23<00:00,  1.57it/s]\n","Epochs 1/8. Running Loss:    2.2879: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 413/413 [04:07<00:00,  1.67it/s]\n","Epochs 2/8. Running Loss:    2.1958: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 413/413 [04:07<00:00,  1.67it/s]\n","Epochs 3/8. Running Loss:    2.2356: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 413/413 [04:07<00:00,  1.67it/s]\n","Epochs 4/8. Running Loss:    1.8977: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 413/413 [04:07<00:00,  1.67it/s]\n","Epochs 5/8. Running Loss:    1.6054: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 413/413 [04:07<00:00,  1.67it/s]\n","Epochs 6/8. Running Loss:    1.7475: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 413/413 [04:07<00:00,  1.67it/s]\n","Epochs 7/8. Running Loss:    1.1424: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 413/413 [04:07<00:00,  1.67it/s]\n","Epoch 8 of 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [33:17<00:00, 249.74s/it]\n","INFO:simpletransformers.t5.t5_model: Training of t5-small model complete. Saved to outputs/.\n","INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3714/3714 [00:14<00:00, 257.81it/s]\n","INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-small_cached_5123714\n","Running Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 465/465 [00:24<00:00, 18.96it/s]\n","INFO:simpletransformers.t5.t5_model:{'eval_loss': 2.2639486075729454}\n","Generating outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.52it/s]\n","Decoding outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.89s/it]\n"]},{"name":"stdout","output_type":"stream","text":["['Rococo Rococo: a classical and classical style of architecture, art and decoration']\n"]},{"name":"stderr","output_type":"stream","text":["Generating outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10.45it/s]\n","Decoding outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.86s/it]\n"]},{"name":"stdout","output_type":"stream","text":["['The Ca Rezzonico: a Venetian Renaissance']\n"]}],"source":["import logging\n","\n","import pandas as pd\n","from simpletransformers.t5 import T5Model\n","\n","logging.basicConfig(level=logging.INFO)\n","transformers_logger = logging.getLogger(\"transformers\")\n","transformers_logger.setLevel(logging.WARNING)\n","\n","train_df['prefix'] = \"summarize\"\n","eval_df['prefix'] = \"summarize\"\n","\n","\n","model_args = {\n","    \"reprocess_input_data\": True,\n","    \"overwrite_output_dir\": True,\n","    \"max_seq_length\": 512,\n","    \"train_batch_size\": 36,\n","    \"num_train_epochs\": 8,\n","}\n","\n","# Create T5 Model\n","model = T5Model(model_name=\"t5-small\", model_type=\"t5\", args=model_args, use_cuda=True)\n","\n","# Train T5 Model on new task\n","model.train_model(train_df)\n","\n","# Evaluate T5 Model on new task\n","results = model.eval_model(eval_df)\n","\n","# Predict with trained T5 model\n","#print(model.predict([\"convert: four\"]))\n","\n","actual_abstract = [\"summarize: \"+\"\"\"Rococo Rococo (/rÉ™ËˆkoÊŠkoÊŠ/, also US: /ËŒroÊŠkÉ™ËˆkoÊŠ/), less commonly Roccoco or Late Baroque, is an exceptionally ornamental and theatrical style of architecture, art and decoration which combines asymmetry, scrolling curves, gilding, white and pastel colors, sculpted molding, and trompe-l'Å“il frescoes to create surprise and the illusion of motion and drama. 8][9] In the late 17th and early 18th century rocaille became the term for a kind of decorative motif or ornament that appeared in the late Style Louis XIV, in the form of a seashell interlaced with acanthus leaves.\"\"\"]\n","\n","print(model.predict(actual_abstract))\n","\n","actual_abstract = [\"summarize: \"+\"\"\"Venetian commodes imitated the curving lines and carved ornament of the French rocaille, but with a particular Venetian variation; the pieces were painted, often with landscapes or flowers or scenes from Guardi or other painters, or Chinoiserie, against a blue or green background, matching the colours of the Venetian school of painters whose work decorated the salons. 24] Ceiling of church of Santi Giovanni e Paolo in Venice, by Piazzetta (1727) Juno and Luna by Giovanni Battista Tiepolo (1735â€“45) Murano glass chandelier at the Ca Rezzonico (1758) Ballroom ceiling of the Ca Rezzonico with ceiling by Giovanni Battista Crosato (1753) In church construction, especially in the southern German-Austrian region, gigantic spatial creations are sometimes created for practical reasons alone, which, however, do not appear monumental, but are characterized by a unique fusion of architecture, painting, stucco, etc. ,.\"\"\"]\n","\n","print(model.predict(actual_abstract))\n","\n","pickle.dump(model, open('../models/title-generator-t5-arxiv-{}-{}.pkl'.format(model_args['train_batch_size'], model_args['num_train_epochs']), 'wb'))"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14855/14855 [00:15<00:00, 961.38it/s] \n","INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-small_cached_51214855\n","INFO:simpletransformers.t5.t5_model: Training started\n","Epochs 0/4. Running Loss:    2.6198: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 465/465 [04:33<00:00,  1.70it/s]\n","Epochs 1/4. Running Loss:    2.2277: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 465/465 [04:29<00:00,  1.72it/s]\n","Epochs 2/4. Running Loss:    1.9162: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 465/465 [04:30<00:00,  1.72it/s]\n","Epochs 3/4. Running Loss:    1.7281: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 465/465 [04:29<00:00,  1.72it/s]\n","Epoch 4 of 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [18:04<00:00, 271.07s/it]\n","INFO:simpletransformers.t5.t5_model: Training of t5-small model complete. Saved to outputs/.\n","INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3714/3714 [00:15<00:00, 244.44it/s]\n","INFO:simpletransformers.t5.t5_utils: Saving features into cached file cache_dir/t5-small_cached_5123714\n","Running Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 465/465 [00:26<00:00, 17.55it/s]\n","INFO:simpletransformers.t5.t5_model:{'eval_loss': 2.152868740276624}\n","Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]C:\\Users\\E\\anaconda3\\envs\\slide-gen\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3426: FutureWarning: \n","`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n","`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n","your targets.\n","\n","Here is a short example:\n","\n","model_inputs = tokenizer(src_texts, ...)\n","with tokenizer.as_target_tokenizer():\n","    labels = tokenizer(tgt_texts, ...)\n","model_inputs[\"labels\"] = labels[\"input_ids\"]\n","\n","See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n","For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n","\n","  warnings.warn(formatted_warning, FutureWarning)\n","Generating outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.18it/s]\n","Decoding outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.99s/it]\n"]},{"name":"stdout","output_type":"stream","text":["['Rococo Rococo Rococo: a classic style of architecture, art and']\n"]},{"name":"stderr","output_type":"stream","text":["Generating outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.90it/s]\n","Decoding outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.92s/it]\n"]},{"name":"stdout","output_type":"stream","text":["['The commodes of the Venetian painters, the cosmosphere,']\n"]}],"source":["import logging\n","\n","import pandas as pd\n","from simpletransformers.t5 import T5Model\n","\n","logging.basicConfig(level=logging.INFO)\n","transformers_logger = logging.getLogger(\"transformers\")\n","transformers_logger.setLevel(logging.WARNING)\n","\n","train_df['prefix'] = \"summarize\"\n","eval_df['prefix'] = \"summarize\"\n","\n","\n","model_args = {\n","    \"reprocess_input_data\": True,\n","    \"overwrite_output_dir\": True,\n","    \"max_seq_length\": 512,\n","    \"train_batch_size\": 32,\n","    \"num_train_epochs\": 4,\n","}\n","\n","# Create T5 Model\n","model = T5Model(model_name=\"t5-small\", model_type=\"t5\", args=model_args, use_cuda=True)\n","\n","# Train T5 Model on new task\n","model.train_model(train_df)\n","\n","# Evaluate T5 Model on new task\n","results = model.eval_model(eval_df)\n","\n","# Predict with trained T5 model\n","#print(model.predict([\"convert: four\"]))\n","\n","actual_abstract = [\"summarize: \"+\"\"\"Rococo Rococo (/rÉ™ËˆkoÊŠkoÊŠ/, also US: /ËŒroÊŠkÉ™ËˆkoÊŠ/), less commonly Roccoco or Late Baroque, is an exceptionally ornamental and theatrical style of architecture, art and decoration which combines asymmetry, scrolling curves, gilding, white and pastel colors, sculpted molding, and trompe-l'Å“il frescoes to create surprise and the illusion of motion and drama. 8][9] In the late 17th and early 18th century rocaille became the term for a kind of decorative motif or ornament that appeared in the late Style Louis XIV, in the form of a seashell interlaced with acanthus leaves.\"\"\"]\n","\n","print(model.predict(actual_abstract))\n","\n","actual_abstract = [\"summarize: \"+\"\"\"Venetian commodes imitated the curving lines and carved ornament of the French rocaille, but with a particular Venetian variation; the pieces were painted, often with landscapes or flowers or scenes from Guardi or other painters, or Chinoiserie, against a blue or green background, matching the colours of the Venetian school of painters whose work decorated the salons. 24] Ceiling of church of Santi Giovanni e Paolo in Venice, by Piazzetta (1727) Juno and Luna by Giovanni Battista Tiepolo (1735â€“45) Murano glass chandelier at the Ca Rezzonico (1758) Ballroom ceiling of the Ca Rezzonico with ceiling by Giovanni Battista Crosato (1753) In church construction, especially in the southern German-Austrian region, gigantic spatial creations are sometimes created for practical reasons alone, which, however, do not appear monumental, but are characterized by a unique fusion of architecture, painting, stucco, etc. ,.\"\"\"]\n","\n","print(model.predict(actual_abstract))"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["pickle.dump(model, open('../models/title-generator-t5-arxiv-{}-{}.pkl'.format(model_args['train_batch_size'], model_args['num_train_epochs']), 'wb'))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":4}
