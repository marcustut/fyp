{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_sentences( article: str) -> 'list[str]':\n",
    "    '''Gets individual sentences for text chunking.'''\n",
    "    sentences = article.split('<eos>')\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def __chunk_text(sentences: 'list[str]') -> 'list[str]':\n",
    "    '''Chunks text for each chunk to be less than the max length.'''\n",
    "    current_chunk = 0\n",
    "    chunks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(chunks) == current_chunk + 1:\n",
    "            # Check if the chunk is less than max_chunk\n",
    "            if len(chunks[current_chunk]) + len(sentence.split()) <= 250:\n",
    "                chunks[current_chunk].extend(sentence.split())\n",
    "            # Next chunk\n",
    "            else:\n",
    "                current_chunk += 1\n",
    "                chunks.append(sentence.split())\n",
    "        else:\n",
    "            chunks.append(sentence.split())\n",
    "\n",
    "    for chunk_id in range (len(chunks)):\n",
    "        chunks[chunk_id] = ' '.join(chunks[chunk_id])\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def __add_tokens(text: str) -> str:\n",
    "    '''Adds tokens to text for easier processing.'''\n",
    "    text = text.replace('.', '.<eos>')\n",
    "    text = text.replace('!', '!<eos>')\n",
    "    text = text.replace('?', '?<eos>')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "article = ''\n",
    "with pdfplumber.open('../program/uploads/SupportLetter.pdf') as pdf:\n",
    "    for page in pdf.pages:\n",
    "        article += ' '.join(((page.extract_text(layout=False)).replace('\\n', '')).split())\n",
    "\n",
    "article = __add_tokens(text=article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = __get_sentences(article=article)\n",
    "chunks = __chunk_text(sentences=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import BartForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.pipelines.base import Pipeline\n",
    "from summarizer import Summarizer\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "checkpoint = 'sshleifer/distilbart-cnn-12-6'\n",
    "model = BartForConditionalGeneration.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "summarizer = pipeline(\n",
    "            'summarization', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = ''\n",
    "with open('../program/uploads/sample1.txt', 'r', encoding='utf-8') as f:\n",
    "    article += ' '.join(((f.read())).replace('\\n', ' ').split())\n",
    "\n",
    "article = __add_tokens(text=article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = __get_sentences(article=article)\n",
    "chunks = __chunk_text(sentences=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = summarizer(chunks, return_text='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' The day I picked my dog up from the pound was one of the happiest days of both of our lives . \"Looking for houses was supposed to be a fun and exciting process. Unfortunately, none of the ones that we saw seemed to match the specifications that we had established. They were too small, too impersonal, too close to the neighbors. After days of finding nothing even close, we began to wonder: was there really a perfect house out there for us? \"'}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary_text': ' Industrial Training Programme offered by the Faculty of Computing and Information Technology, Tunku Abdul Rahman University College (TAR UC) The main objective of the industrial training programme is to provide students with practical training opportunities in one or more of the following areas . We believe with the expert guidance and experience of your esteemed organisation, our students will acquire relevant practical skills and experience which would be valuable to the students later in their working life .'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_model = pickle.load(open('../models/title-generator-t5-arxiv-16-4.pkl', 'rb'))\n",
    "title_summarizer = title_model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __transpose_dict(dict_: dict) -> dict:\n",
    "    '''Transposes the keys and values of the dictionary object. Based on the assumption that all keys and values are unique.'''\n",
    "    return {y:x for x, y in dict_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating outputs: 100%|██████████| 65/65 [00:06<00:00, 10.20it/s]\n",
      "Decoding outputs: 100%|██████████| 65/65 [00:14<00:00,  4.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Industri',\n",
       " 'Al Train al Train',\n",
       " 'Prog ing Prog ing Prog ing Prog ing Prog',\n",
       " 'Ramme of ramme of ramme of',\n",
       " 'Fered by fered by fered by fered by fered by',\n",
       " 'Facing the Facc Fac',\n",
       " 'ulty of ulty of ulty of',\n",
       " 'Computin Computin',\n",
       " 'In g and Infrared',\n",
       " 'Formatio',\n",
       " 'Technological n Technological n Technological',\n",
       " 'logy, Tu Tu logy, Tu Tu Tu Tu Tu Tu Tu Tu Tu Tu Tu',\n",
       " 'Abdudu nku Abdu Abdu nku Abdu Abdu',\n",
       " 'Rahman Rahman',\n",
       " \"Universit's Universit'es Universit'e\",\n",
       " 'ity Coll Coll Coll Coll',\n",
       " 'Ege (TAR): a ege (TAR)',\n",
       " 'UC Theorems',\n",
       " 'Main obstructor ob',\n",
       " 'jective jective jive',\n",
       " 'I',\n",
       " 'ndustria ndustria',\n",
       " 'Traini l traini',\n",
       " 'ng progr ng progr ng progr ng prog',\n",
       " 'Amme isotropic and amme isotropic',\n",
       " 'Proviate Proviant Proviant Proviant',\n",
       " 'De stude em es em es em',\n",
       " 'nts with nts with nts',\n",
       " 'Practic practicum',\n",
       " 'Al train',\n",
       " 'Oppospos oppos oppos oppos oppos oppos',\n",
       " 'Rtunitie rtunitie',\n",
       " 'One s s s a s a s',\n",
       " 'Or more: A note on the number of adobes',\n",
       " 'The',\n",
       " 'Followin followin',\n",
       " 'G areas in g areas',\n",
       " 'We belgian eigente eigen eigen eigen',\n",
       " 'Ieve wit wit',\n",
       " 'Extinction of ex-ex ex ex ex ex ex ex ex ex ex ex ex',\n",
       " 'Pert guido guido guido pert guis',\n",
       " 'Dance an a',\n",
       " 'Experimentation of experimentation d experimentation',\n",
       " 'Censoredence of ence of ence of ence of',\n",
       " 'Your estrangement a tuomo',\n",
       " 'Or, or, or',\n",
       " 'Ganisatisati ganisatisati',\n",
       " 'On, our planetary counterparts on, our planetary counterpart',\n",
       " 'Students',\n",
       " 'Acquiring the acquitter of willac will acquit',\n",
       " 'Quire re re re re re re re re',\n",
       " 'Levant p p p p p p p p',\n",
       " 'Rractical ractical ractical',\n",
       " 'Skills Skills Skills Skills Skills Skills Skills Skills Skills Skills Skills Skills Skills Skills Skills Skills Skills Skills Skills',\n",
       " 'Exercissing with and expelling expective expees',\n",
       " 'Riminice w effrce w ef',\n",
       " 'hich wou hich wou hich wou hich wo',\n",
       " 'Vacuum vasculature vasculature',\n",
       " 'tudos tudos tudos',\n",
       " 'o the stt st tst o st',\n",
       " \"Udents ltudos l'es l'\",\n",
       " 'Ater in the ater in a aer in a aer in',\n",
       " 'Their worry worry',\n",
       " 'Rking li rking li',\n",
       " 'Fe fe fe fe.']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_summarizer(\"Industrial Training Programme offered by the Faculty of Computing and Information Technology, Tunku Abdul Rahman University College (TAR UC) The main objective of the industrial training programme is to provide students with practical training opportunities in one or more of the following areas . We believe with the expert guidance and experience of your esteemed organisation, our students will acquire relevant practical skills and experience which would be valuable to the students later in their working life .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating outputs:   0%|          | 0/56 [00:00<?, ?it/s]C:\\Users\\E\\anaconda3\\envs\\slide-gen\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3426: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "Generating outputs: 100%|██████████| 56/56 [00:05<00:00, 10.31it/s]\n",
      "Decoding outputs: 100%|██████████| 56/56 [00:15<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'The day of the day': ' The day I picked my dog up from the pound was one of the happiest days of both of our lives . \"Looking for houses was supposed to be a fun and exciting process. Unfortunately, none of the ones that we saw seemed to match the specifications that we had established. They were too small, too impersonal, too close to the neighbors. After days of finding nothing even close, we began to wonder: was there really a perfect house out there for us? \"'}]\n"
     ]
    }
   ],
   "source": [
    "new_results2 = []\n",
    "# results\n",
    "\n",
    "for i in range(len(results)):\n",
    "    body = results[i]['summary_text']\n",
    "    new_results2.append({title_summarizer(body)[0]: body})\n",
    "\n",
    "print(new_results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating outputs: 100%|██████████| 50/50 [00:04<00:00, 10.92it/s]\n",
      "Decoding outputs: 100%|██████████| 50/50 [00:17<00:00,  2.86it/s]\n",
      "Generating outputs: 100%|██████████| 44/44 [00:03<00:00, 11.08it/s]\n",
      "Decoding outputs: 100%|██████████| 44/44 [00:14<00:00,  2.97it/s]\n",
      "Generating outputs: 100%|██████████| 44/44 [00:04<00:00, 10.07it/s]\n",
      "Decoding outputs: 100%|██████████| 44/44 [00:14<00:00,  2.96it/s]\n",
      "Generating outputs: 100%|██████████| 41/41 [00:03<00:00, 10.96it/s]\n",
      "Decoding outputs: 100%|██████████| 41/41 [00:14<00:00,  2.77it/s]\n",
      "Generating outputs: 100%|██████████| 39/39 [00:03<00:00, 10.36it/s]\n",
      "Decoding outputs: 100%|██████████| 39/39 [00:14<00:00,  2.62it/s]\n",
      "Generating outputs: 100%|██████████| 53/53 [00:04<00:00, 10.64it/s]\n",
      "Decoding outputs: 100%|██████████| 53/53 [00:15<00:00,  3.53it/s]\n",
      "Generating outputs: 100%|██████████| 49/49 [00:05<00:00,  9.40it/s]\n",
      "Decoding outputs: 100%|██████████| 49/49 [00:15<00:00,  3.24it/s]\n",
      "Generating outputs: 100%|██████████| 54/54 [00:04<00:00, 11.08it/s]\n",
      "Decoding outputs: 100%|██████████| 54/54 [00:15<00:00,  3.60it/s]\n",
      "Generating outputs: 100%|██████████| 50/50 [00:04<00:00, 11.04it/s]\n",
      "Decoding outputs: 100%|██████████| 50/50 [00:14<00:00,  3.35it/s]\n",
      "Generating outputs: 100%|██████████| 50/50 [00:05<00:00,  9.12it/s]\n",
      "Decoding outputs: 100%|██████████| 50/50 [00:15<00:00,  3.32it/s]\n",
      "Generating outputs: 100%|██████████| 76/76 [00:07<00:00,  9.85it/s]\n",
      "Decoding outputs: 100%|██████████| 76/76 [00:15<00:00,  5.00it/s]\n",
      "Generating outputs: 100%|██████████| 45/45 [00:04<00:00, 10.40it/s]\n",
      "Decoding outputs: 100%|██████████| 45/45 [00:14<00:00,  3.00it/s]\n",
      "Generating outputs: 100%|██████████| 45/45 [00:04<00:00, 10.78it/s]\n",
      "Decoding outputs: 100%|██████████| 45/45 [00:15<00:00,  3.00it/s]\n",
      "Generating outputs: 100%|██████████| 49/49 [00:04<00:00, 10.91it/s]\n",
      "Decoding outputs: 100%|██████████| 49/49 [00:15<00:00,  3.25it/s]\n",
      "Generating outputs: 100%|██████████| 40/40 [00:03<00:00, 10.85it/s]\n",
      "Decoding outputs: 100%|██████████| 40/40 [00:14<00:00,  2.69it/s]\n",
      "Generating outputs: 100%|██████████| 29/29 [00:02<00:00, 13.02it/s]\n",
      "Decoding outputs: 100%|██████████| 29/29 [00:14<00:00,  1.96it/s]\n",
      "Generating outputs: 100%|██████████| 28/28 [00:02<00:00, 11.65it/s]\n",
      "Decoding outputs: 100%|██████████| 28/28 [00:14<00:00,  1.89it/s]\n",
      "Generating outputs: 100%|██████████| 55/55 [00:05<00:00, 10.59it/s]\n",
      "Decoding outputs: 100%|██████████| 55/55 [00:14<00:00,  3.69it/s]\n",
      "Generating outputs: 100%|██████████| 38/38 [00:03<00:00, 10.32it/s]\n",
      "Decoding outputs: 100%|██████████| 38/38 [00:14<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_results: [{'Using the e-commerce method to analyze the use of a p-value': ' The user has requested enhancement of the downloaded file . See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/357876127Artiﬁcial Intelligence Art: Attitudes and Perceptions Toward Human Versus Artificial Intelligence Artworks . The research is a study on the young generation views and acceptance of Artificial Intelligence (AI) art .'}, {'Ternary Ternoids': ' The term Artificial Intelligence (AI) originated in the 1950’s in modelling human cognition . Nowadays, the term has evolved to refer to application that rely on deep neural networks . AI art refers to artwork made by collaboration between AI algorithms and human artists . 54% of respondents did not correctly identify emotions in AI artworks .'}, {'A pamphlet on the paternal pamphlet': ' This paper is constructed in four main sections: Literature Review, Research Methodology, Research Results and Discussion, and Conclusion . According to previous studies, AI can be utilized as Imitator, Collaborator, and Creator . On the other hand, humans might not be able to appreciate AI artworks emotionally as they treated it as artificial .'}, {'2018 - The 2018 XXII': ' In 2018, a painting as the product of human-AI collaboration was auctioned for $432,500 (approximately RM1,716,022) at a British auction house . The painting was created using Generative Adversarial Networks (GANs) The goal of AI as a Collaborator is to create novel artworks itself, though with heavy human involvement .'}, {'Elgamma Elgamma Elgamma Elgamma Elgamm': ' Elgammal proposed a new art-generating system named Creative Adversarial Network (CAN) CAN is an adaptation of GANs where it also uses the generator and discriminator networks . The difference lies in the training, where the discriminator instead of generator is trained to recognize the styles of artworks .'}, {'A note on a sembly of a p-epoch': ' There are three types of creativity: combinational, exploratory and transformational creativity . Combinational creativity generates new combinations from familiar ideas by identifying indirect associations between two concepts from random sources . Poem. exe generates and posts unique Japanese Haiku poems on Twitter through random combinations of data input . AI algorithms face challenges in combinational creativity .'}, {'Genetic Genetic Genetic Genetic Genetic Genetic Genetic Genetic Genetic Genetic Genetic Genetic Genetic Genetic Genetic Genetic Genetic Genetic Genetic': ' Genetic Algorithms (GAs) can be incorporated into the program, which allows random changes that are similar to mutations in biological evolution . Sculptor William Latham cautiously and sparingly employed GAs in his AI program, eliminating large point mutations altogether . As a result, the series of produced images was artworks beyond himself but remains similar in his artistic style .'}, {'Recepti Recepti Recepti Recepti Recepti Recepti Re': ' Reception of AI paintings was positive when human subjects were unaware of the painter’s identity [24, 26] When people were told certain paintings were created by AI (attributed artist identity = AI), they rated the paintings significantly lower than other people who thought the artist was human . Therefore, we want to replicate the experiment and find out how attributed artist identity might affect the judgement on AI art .'}, {'Section Section Section Section Section': \" Section 3 is a Turing test which is used to test whether respondents could pick the correct artwork produced by AI . Section 4 is about personal opinion which elicit the degree of exposure to AI technology and respondents’ acceptance of AI artworks . Section 5 was an experimental design to find out the relationship between attributed artist identity and respondents' judgement on AI artwork .\"}, {'Pilot pilot pilot pilot': ' A pilot test is conducted on five college students before the questionnaire is distributed online through social media, email and WhatsApp . The reliability of the questionnaire items is also tested using Cronbach’s Alpha test utilizing PSPP with the result of N = 47, α = 87, which indicates a high reliability . Table 1 shows a total of 202 respondents demographics: age, gender, and faculty .'}, {'The equivariant equivariant equivalence': ' In the questionnaire Section 2, the portraits in questions 1 and 2 are the same images but modified with different color palettes . Respondents are required to choose the group of phrases that closely resembled what they felt from the paintings . If the chosen group of emotions did not correspond to the intended emotion, the response is counted as “emotion not correctly identified” Result of the experiment shows that 54% (mean) of the undergraduates cannot identify the intended emotions correct . This shows that the advancement of AI artworks, human’s apprehension of the artworks is not aligned .'}, {'Based Based octro-eff o es e': ' Based on Table 3, respondents have moderate exposure to AI technology (Mean = 3. 33) Table 4 shows the acceptance rate of AI artworks and the mean is 74. 9% However, based on Table 5, there is no significant correlation between degree of exposure and acceptance . This result is different from the theory proposed by Zajonc’s “Mere Exposure Effect” [12]'}, {'Ba Q3: Ba Q3: “Baaaaaaa': ' Q3: “Based on your personal view, what role do you think is most suitable for AI in the Positive Response creative arts industry? ” (95. 5%) Q4. “Which word describes your feeling about contemporary AI artwork most Positive Response accurately? “ (80. 7%) MEANS 74. 9%) H3: Undergraduates are able to correctly identify human artwork versus AI artworks .'}, {'A note on the equivalence of the epoch': ' In this experimental test, respondents are informed that the images in a subsection are created by AI . Respondents have to judge the images accordingly . For each attributed identity group, the human artist identity group consistently gain slightly higher ratings . Based on the independent Sample T-Test, there is a relationship between attributed artwork and judgement on AI artwork .'}, {'A note on the hadron of hadrons': ' This has proven that researchers can focus in improving AI algorithm in order to produce more realistic artworks . Acceptance and judgement of AI artwork is not based on one’s AI knowledge or exposure to AI as arts is a field of feelings and it depends on different individuals’ experience and personal preference .'}, {'N-Security of P. R. N': ' P. R. N. Childs, P. C. and R. M. N . (2019) Mechanical Design Engineering Handbook. Elsevier. (P.C. N) Childs: A Computational Approach for Combinational Creativity in Design. In Proceedings of the DESIGN 2018, pp. 1815-1824.'}, {'Can GPT GPT GPT GPT GPT GPT GPT GPT GPT': \" Can GPT-3 Pass a Writer’s Turing Test? Journal of Cultural Analytics. (2020) Elkins, K. Elkins and Chun, J. (2019) Can the GPT 3 pass a Writer's Turing test? Can the new algorithm pass the Turing test of a writer's work?\"}, {'A. Hong': ' A. Hong, J. W. and Curran, N. M. (2019) Artificial Intelligence, Artists, and Art: Attitudes Toward Artwork Produced by Humans vs. Artificial Intelligence . Jennings, Jennings, E. E. (2010) Developing Creativity: Artificial Barriers in Artificial Intelligence. Minds and Machines,20(4), pp. 489–501. Jennings, K. E., M. Mazzone, M. Elgammal, A. Kolliopoulos, C. Martindale (1990) The Clockwork Muse: The Predictability Of Artistic Change.'}, {'Design Design': ' Design Studies: Assessing Design Creativity: Refinements to the Novelty assessment Method, Design Studies, 32(4), pp. 348–383 . Sawyer, R. K. Sawyer, K. and Chakrabarti, A. Sundararajan, L. (2014) Mind, Machine, and creativity: An Artist’s Perspective. Journal of Creative Behavior, 48(2), pp 136–151.'}]\n"
     ]
    }
   ],
   "source": [
    "new_results = []\n",
    "for dict_ in results:\n",
    "    # Now the body is the key and the title is the value\n",
    "    dict_ = __transpose_dict(dict_)\n",
    "    for body in dict_:\n",
    "        # Summarize the given text\n",
    "        dict_[body] = title_summarizer(body)[0] # Returns a list so take only the first element\n",
    "        pass\n",
    "    dict_ = __transpose_dict(dict_)\n",
    "    new_results.append(dict_)\n",
    "print('new_results:', new_results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "674d0c5ad507fdf0f49a51b6f7a757e543757995548e180cf9d61bc60776206c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('slide-gen')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
