{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title Generator Using T5\n",
    "\n",
    "References: [Here](https://shivanandroy.com/transformers-generating-arxiv-papers-title-from-abstracts/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = '../dataset/arxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "def get_metadata():\n",
    "    with open(data, 'r') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Calculation of prompt diphoton production cross sections at Tevatron and\n",
      "  LHC energies\n",
      "\n",
      "Abstract:   A fully differential calculation in perturbative quantum chromodynamics is\n",
      "presented for the production of massive photon pairs at hadron colliders. All\n",
      "next-to-leading order perturbative contributions from quark-antiquark,\n",
      "gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\n",
      "all-orders resummation of initial-state gluon radiation valid at\n",
      "next-to-next-to-leading logarithmic accuracy. The region of phase space is\n",
      "specified in which the calculation is most reliable. Good agreement is\n",
      "demonstrated with data from the Fermilab Tevatron, and predictions are made for\n",
      "more detailed tests with CDF and DO data. Predictions are shown for\n",
      "distributions of diphoton pairs produced at the energy of the Large Hadron\n",
      "Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\n",
      "boson are contrasted with those produced from QCD processes at the LHC, showing\n",
      "that enhanced sensitivity to the signal can be obtained with judicious\n",
      "selection of events.\n",
      "\n",
      "Ref: Phys.Rev.D76:013009,2007\n"
     ]
    }
   ],
   "source": [
    "metadata = get_metadata()\n",
    "\n",
    "for paper in metadata:\n",
    "    paper_dict = json.loads(paper)\n",
    "    print('Title: {}\\n\\nAbstract: {}\\nRef: {}'.format(paper_dict.get('title'), paper_dict.get('abstract'), paper_dict.get('journal-ref')))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take last 5 years of ArXiv papers due to computational resource limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18566, 18566, 18566)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = []\n",
    "abstracts = []\n",
    "years = []\n",
    "metadata = get_metadata()\n",
    "\n",
    "for paper in metadata:\n",
    "    paper_dict = json.loads(paper)\n",
    "    ref = paper_dict.get('journal-ref')\n",
    "    try:\n",
    "        year = int(ref[-4:])\n",
    "        if 2016 < year < 2021:\n",
    "            years.append(year)\n",
    "            titles.append(paper_dict.get('title'))\n",
    "            abstracts.append(paper_dict.get('abstract'))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "len(titles), len(abstracts), len(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 18K research papers published from 2016 to 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On the Cohomological Derivation of Yang-Mills ...</td>\n",
       "      <td>We present a brief review of the cohomologic...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Regularity of solutions of the isoperimetric p...</td>\n",
       "      <td>In this work we consider a question in the c...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asymptotic theory of least squares estimators ...</td>\n",
       "      <td>This paper considers the effect of least squ...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Teichm\\\"uller Structures and Dual Geometric Gi...</td>\n",
       "      <td>The Gibbs measure theory for smooth potentia...</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Distributional Schwarzschild Geometry from non...</td>\n",
       "      <td>In this paper we leave the neighborhood of t...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  On the Cohomological Derivation of Yang-Mills ...   \n",
       "1  Regularity of solutions of the isoperimetric p...   \n",
       "2  Asymptotic theory of least squares estimators ...   \n",
       "3  Teichm\\\"uller Structures and Dual Geometric Gi...   \n",
       "4  Distributional Schwarzschild Geometry from non...   \n",
       "\n",
       "                                            abstract  year  \n",
       "0    We present a brief review of the cohomologic...  2017  \n",
       "1    In this work we consider a question in the c...  2018  \n",
       "2    This paper considers the effect of least squ...  2017  \n",
       "3    The Gibbs measure theory for smooth potentia...  2020  \n",
       "4    In this paper we leave the neighborhood of t...  2018  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "papers = pd.DataFrame({\n",
    "    'title': titles,\n",
    "    'abstract': abstracts,\n",
    "    'year': years\n",
    "})\n",
    "\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The `simpletransformers` library is used to train the `T5 model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14853, 3), (3713, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding input and target columns\n",
    "papers = papers[['title', 'abstract']]\n",
    "papers.columns = ['target_text', 'input_text']\n",
    "\n",
    "# Adding prefix columns\n",
    "papers['prefix'] = 'summarize'\n",
    "\n",
    "# Splitting data using the 80:20 training-testing ratio\n",
    "eval_df = papers.sample(frac=0.2, random_state=673)\n",
    "train_df = papers.drop(eval_df.index)\n",
    "\n",
    "train_df.shape, eval_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530ab78d751649078951df674d258910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14853 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liana/projects/fyp/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3407: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/home/liana/projects/fyp/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3407: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/home/liana/projects/fyp/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3407: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/home/liana/projects/fyp/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3407: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/home/liana/projects/fyp/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3407: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/home/liana/projects/fyp/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3407: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from simpletransformers.t5 import T5Model\n",
    "\n",
    "# Setting logging information\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger('transformers')\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Training parameters\n",
    "params = {\n",
    "    'reprocess_input_data': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'max_seq_length': 512,\n",
    "    'train_batch_size': 16,\n",
    "    'num_train_epochs': 4,\n",
    "    'best_model_dir': '../models/title-generator'\n",
    "}\n",
    "\n",
    "# Creating model\n",
    "model = T5Model(model_type='t5', model_name='t5-small', args=params, use_cuda=True)\n",
    "\n",
    "# Training model\n",
    "model.train_model(train_df)\n",
    "\n",
    "# Evaluating results\n",
    "results = model.eval_model(eval_df)\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1d9160c5522fde555d2c10488cfe9d2801520d7dbc6e01dcd193dab8e0ff234"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
